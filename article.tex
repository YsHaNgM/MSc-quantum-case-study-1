%!TEX program = xelatex
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\usepackage{float}


\usepackage{url}
\bibliographystyle{plain}


\usepackage{listings}

\usepackage{parskip}
\setlength{\parindent}{1em}
\linespread{1.25}

\newcommand\diff{\,\mathrm{d}}
\renewcommand{\footnoterule}{\rule{\linewidth}{0pt}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}
\title{Case study of Scalable Quantum Simulation of Molecular Energies}
\author{Shuhao Yang, James Mills, Shanice St John}
\date{\today}
\maketitle
\section{Introduction}
Richard Feynman’s words are as true today as when he spoke them: “Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical, and by golly it's a wonderful problem, because it doesn't look so easy.” 

Simulation of physical systems is one of the most crucial practical applications of computation by far.  On the macroscopic scale of our lives, for instance, buildings are engineered safe and cost efficient using ﬁnite element analysis and modeling. Aeronautics rely hugely on computational ﬂuid dynamics simulations. Even nuclear weapons are tested by exhaustive computational modeling.

But even so, classical computation still has its limits, including cost and accuracy. Additionally, when it comes to simulating microscopic systems, classical technique also exhibits its downside on efficiency. In computational chemistry, each subatomic orbitals and their interactions needs to be considered to achieve an accurate and realistic simulation of the microscopic system. Since the state of one orbital can be affected by all of the others, an exponential number of states in the basis of molecular orbitals should be computed. This is what we call ‘many body’ problems. No matter how far apart two orbitals are, their quantum states cannot be described independently, the repeated interactions between particles create quantum correlations, I.e., entanglement. 

A more advanced computation technique is then employed, quantum computation. It harnesses the principles of quantum mechanics, hence solves the classically intractable problems more accurately, faster, with a fine-grained simulation. 

For those information in our classical computers enough to fill up the entire memory, instead, a quantum computer can only use a few quantum bits, or qubits to store them \cite{trabesinger2012quantum}. since the classical storage unit ‘bit’ is binary system base on 0 and 1, but quantum computer encode them into two distinguishable quantum states, and one qubit can represent multiple states once by quantum phenomenon ‘superposition’. similarly, if we perform a searching algorithm on a classical string, the enumerate steps would be the number of bits within this string. comparably, our processing step will reduce a lot more as we only need to compute once in a quantum processor.

\section{History}
Molecules consist of various atoms and the elementary unit of an atom is electron and nucleus. When we consider atomic energy, electron-nuclei, electron-electron interactions must be examined. Molecular energy consists of interactions between multiple atoms, including interactions between electrons from different atoms, as well as nuclei. Molecular energy can be pictured as a large package that contains many small balls, and the sum of the energy of their collisions is the sum of interactions between atoms. 

Obviously, it demands massive computation to find out the package energy as it contains a huge amount of particles. Therefore, some approximations need to be made to simply our model, hence molecular mechanics is introduced.

Molecular mechanics models molecular systems using force fields in classical mechanics. Three approximations were made to reduce the computational complexity. Each atom simulated as one particle; each particle has their designated attributes including radius, Polaris ability and a net charge; and international are treated as springs with an equilibrium distance same as bond length. This theory essentially simulated every element of the molecular system as fixed particles connected by strings, which largely reduces the computational complexity.

A similar theory of reducing the amount of computation is called renormalisation group. A simple scenario is to calculate the resistance force of an iron ball in a glass of water. We can simply simulate every water molecule in the glass by inputting the real size of a single water molecule as one parameter. The sizes of the molecules can easily be another 1000 parameters as they can be quite irregular and difficult to describe. Forces from multiple directions of the iron ball could be another 1000 parameters. Inputting all parameters, modelling an iron ball and simulating $10^26$ water molecules and their interactions can give the resistance force. Let’s say we have a supercomputer that allows tremendous amount of data processing and the result that this simulation gives coincides with experimental data perfectly.

But what if we want to reduce the computational complexity? One solution is to reduce the number of water molecule simulated, which is feasible given the relative bigger size of the iron ball. To make up for the loss in number of molecules, the rest parameters can be changed. Let’s say we manipulated the parameters and the simulation gives slightly different solutions, but the overall trend is still the same as before. 

The computation complexity can still be compressed. We can repeat the process of reducing the number of molecules and adjusting the parameters, and still generate acceptable solutions. 

This process is called renormalisation group, and the results that this model generates is called low energy effective theory, in which the system size is reduced but the results is still acceptable and effective. 

We now return to the theory of molecular mechanics, it is not exactly accurate since it is restricted by parameters of equations, ie difference force fields for different atoms. A similar and perhaps more comprehensible explanation can be made from the theory of renormalisation group. While decreasing the parameters of calculation and reducing computation time, the accuracy of the result is decreasing as well. Molecular mechanics does the same for force fields and interactions inside the atom. 

Overall, the work on simulation molecular energies with classical methods is a trade off between precision and cost. Quantum simulation is able to contain all information relevant to the system and simulate them exactly as reality without approximations.


\section{Non-technical background}
In any quantum computing processes, the building blocks of the process are qubits. A qubit, or quantum bit, is the quantum analogue of the classical bit (strings of zeros and ones). So the qubit represents a quantum superposition or combination of a predetermined ‘0’ and a ‘1’ state. The quantum algorithms were run on superconducting qubits, these are a physical realisation of quantum bits based upon what are known as superconducting interference devices (SQUIDs), and attempt to find properties of the wavefunction of the molecule (in this case molecular Hydrogen).

The wavefunction of a quantum system is a mathematical construct describing the state, it provides probabilities for results of particular measurements made on the state. In quantum chemistry the electronic structure of a molecule is described by the movement of the electrons of a collection of fixed atomic nuclei. And so a comprehensive description of the electronic structure encompasses the wavefunctions of all of these electrons, along with their respective energies. The idea that the motions of the electrons and nuclei may be considered separately is known as the Born-Oppenheimer approximation. It is a very useful assumption that simplifies electronic structure calculations appreciably. 

One of the reasons that developing new approaches to tackling electronic structure calculations are of importance is that they are extremely computationally expensive. Indeed one of the original motivations for the field of quantum computing was the desire to simulate quantum systems in nature by using quantum systems. A prominent driving factor in the interest in and development of quantum computing is the aim to create quantum hardware and software that will allow for efficient quantum simulations of chemical systems. A problem of tantamount importance in quantum chemistry is that of finding the lowest energy eigenvalue of molecular electronic structures. These energy eigenvalues define a great deal of properties of interest in the molecule. This is of great interest in for example the development of pharmaceuticals where companies have an extremely laborious testing and trials process, if a significant part of the initial testing process could be done in silico - this would represent significant savings and would better ensure that the pharmaceutical under development behaved as required prior to starting trials.
\section{Techniques in paper}
\cite{ryabinkin2018constrained}
\section{Main Achievement of paper}
\section{Conclusion and outlook}
As this is the first scalable simulation of molecular energies on any type of quantum hardware it represents an important step in the history of quantum computing applications. The results indicate that for near-term quantum devices without effective error correction the adaptive approach represented by the Variational Quantum Eigensolver may have better error resilience than that of standard gate model approach represented by the Phase Estimation Algorithm. The expectation is that this work will pave the way for the development of further techniques for intractable calculations in quantum chemistry. And the final aim is that eventually these variational quantum chemistry techniques will allow for numerically exact simulations of chemical reaction rates, which are of great industrial commercial value. In particular, the efficiency - requiring only short state preparation and measurement procedures, and so it is without the normal substantial overhead required for quantum error correction - and robustness to error displayed by the experimental implementation of the variational quantum eigensolver technique indicates that this technique might succeed where classical techniques fail in solving traditionally intractable problems. 

Late last year the US passed the new National Quantum Initiative Act pledging around \$1.2 billion in funding for quantum information science. At a similar time the EU commission launched the €1 billion Quantum Technologies Flagship which will fund over 5000 of the top European researchers in quantum technologies over the next decade. In the UK, the EPSRC-backed Quantum Technology Hubs represent part of a £270 million investment to create national networks of quantum expertise. With this extraordinary push of late across Europe and also in the US and China, aiming to fast-track quantum technologies out of university research groups and into industrial applications, the importance of this work in quantum chemistry simulations cannot be overstated. As aside from the much touted almost pop-science status of quantum encryption, the myriad opportunities afforded by more efficient scalable quantum chemistry simulations looms large in garnering serious investor interest. The ability to run quantum simulations of the behaviour of molecules and materials is hoped offer new avenues for increasing our understanding of chemistry and enable the development of new medicines and materials in silico.

\bibliography{ref}
\end{document}